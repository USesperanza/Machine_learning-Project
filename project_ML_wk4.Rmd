---
title: "Machine Learning_HAR_Weight Lifting"
author: "Maneesh Pandey"
date: "April 1, 2018"
output: html_document
---
## Overview:
The project work is based on the paper "Qualitative Activity Recognition of Weight Lifting Exercises" The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har." The authors of this paper using the sensors attached to body of the participants studied "qualitatve"(how well)the excercise was performed, in this case weightlifting using a dumbell of 1.25kg.
 In this project, goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
 Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in ???ve di???erent fashions: exactly according to the speci???cation (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the speci???ed execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate.
The aim of this work is to investigate the feasibility of automatically assessing the quality of execution of weight lifting exercises and the impact of providing real-time feedback to the Participant. - so-called qualitative activity recognition. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(AppliedPredictiveModeling)
library(caret)
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(dplyr)
library(gbm)
library(lubridate)
library(forecast)
library(e1071)
library(randomForest)
library(gam)
```
```{r,echo=TRUE}
#importing training data
url1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
loc1<-("~/analytics_case study/machineLearning/training.csv")
download.file(url1,loc1)
#importing testing data
url2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
loc2<-("~/analytics_case study/machineLearning/testing.csv")
download.file(url2,loc2)
# reading data
training<-read.csv("training.csv",skipNul =TRUE)
testing<-read.csv("testing.csv", skipNul =TRUE)
table(training$classe)
#training<-training[,(na.rm=TRUE)]
inTrain<-createDataPartition(y=training$classe,p=0.7,list=FALSE)
train_1<-training[inTrain,]
test_1<-training[-inTrain,]
# Removing columns with Near Zero variance in training data, 
nzv<-nearZeroVar(train_1,saveMetrics = TRUE)
train_1<-train_1[,nzv$nzv==FALSE]
# Removing coulmns with Near Zero Variance in test data
nzv1<-nearZeroVar(test_1,saveMetrics = TRUE)
test_1<-test_1[,nzv1$nzv==FALSE]
#removing columns with NA's more than 60% in training data
outtrain<-train_1[,colSums(is.na(train_1)/(nrow(train_1)))>=0.6]
del_cols1<-colnames(outtrain)
train_2<-train_1[,!(names(train_1)%in% del_cols1)]
table(train_2$classe)
#removing X,rawtimestamps,cvtd,numWindow from training and test data
train_2<-train_2[,-c(1,3:6)]
test_2<-test_1[,names(train_2)]
table(test_2$classe)
dim(train_2)
dim(test_2)
test_final<-test_2[,-54]
# mapping testing data with same columns as in training data
testing_2<-testing[,names(test_final)]
dim(testing_2)
```
```{r,echo=TRUE}
# Preprocessing and training with partitinong and classification
set.seed(55555)
train_fit<-rpart(classe~.,data=train_2,method="class")
#print(train_fit)
plot(train_fit,uniform = TRUE, main="Classification tree")
text(train_fit,use.n = TRUE,all=TRUE, cex=0.7)
```

```{r,echo=TRUE}
predict_tree<-predict(train_fit,newdata=test_final,type="class")
confusionMatrix(predict_tree,test_2$classe)
```
The accuracy with this model is 0.7172


```{r,echo=TRUE}
#model fit with Random forest
set.seed(55555)
rf_fit<-randomForest(classe~.,data=train_2, ntree=250, type ="classification")
pre_rf<-predict(rf_fit,newdata=test_final)
confusionMatrix(pre_rf,test_2$classe)

```
The accuracy with Random Forest is 0.9942, with reduced number trees to reduce computation time.

```{r,echo=TRUE}
#model fit with Lda 
set.seed(55555)
lda_fit<-train(classe~.,data=train_2,  method ="lda")
pre_lda<-predict(lda_fit,newdata=test_final)
confusionMatrix(pre_lda,test_2$classe)
```
The Overall accuracy of lda model is 0.731


```{r,echo= TRUE}
#model with gbm 
set.seed(55555)
gbmcontrol<-trainControl(method = "repeatedcv",number=3,repeats=1)
gbm_fit<-train(classe~.,data=train_2,  method ="gbm",trControl=gbmcontrol,verbose=FALSE)
pre_gbm<-predict(gbm_fit,newdata=test_final)
confusionMatrix(pre_gbm,test_2$classe)
```
The accuracy of the GBM model is 0.9641


```{r,echo=TRUE}

table(pre_gbm,pre_lda)
comp_nb_lda<-(pre_gbm==pre_lda)
qplot(pre_gbm,pre_lda,color=comp_nb_lda,data=test_2)

```
```{r, echo=TRUE}
#combining random forest and gbm models
set.seed(55555)
comb_df<-data.frame(pre_gbm,pre_rf,classe=test_2$classe)
comb_modfit<-train(classe~.,data=comb_df,method="gam",verbose=FALSE)
```

```{r,echo=TRUE}
comb_pred<-predict(comb_modfit,comb_df)
confusionMatrix(comb_pred,test_2$classe)
```

Though the sesnitivity has increased, the combined model accuracy has dropped to 0.4771

## summary of model selection
various models have been built to check on improving accuracy and sensitivity for Class A, hence based on this the randomforest model with accuruacy of 0.9969 and sensitivity of 1.0 for Class A is selected for test data.
```{r,echo=TRUE}
predict_final<-predict(rf_fit,newdata=testing_2)
predict_final
```
The above is prediction of Class types for given testing data of 20 sets.



## Exploratory data analysis


```{r, echo=FALSE}
#Plot1
qplot(user_name,total_accel_arm+total_accel_dumbbell+total_accel_forearm+total_accel_belt, colour=classe,data=training)
#density plots
 qplot(classe,colour=user_name,data=training, geom="density")
 
```

